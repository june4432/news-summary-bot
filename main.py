### ğŸ“ main.py
from rss import get_latest_news_urls
from crawler import crawl_news
from fast_news_crawler import crawl_multiple_news  # âœ… ë³‘ë ¬ í¬ë¡¤ë§ í•¨ìˆ˜ë¡œ êµì²´
from summarizer import summarize_news_via_api
from notion_writer import save_to_notion, get_existing_urls_from_notion
from mailer import build_email_body, send_email, get_email_subject
from bulk_mailer_advanced import send_bulk_email
from config import (
    api_key, notion_token, notion_database_id,
    sender_email, sender_app_password,
    recipient_email, notion_url
)
import datetime
from log import logger
import json
import os

logger.info("ë‰´ìŠ¤ë ˆí„° ë°œì†¡ ì‘ì—… ì‹œì‘!!")

BASE_DIR = os.path.dirname(os.path.abspath(__file__))

RECIPIENTS_PATH = "/home/pi/project/news-summary-bot/recipients_email.json"
RSS_SOURCES_FILE = os.path.join(BASE_DIR, "rss_sources.json")

def load_recipients(filepath=RECIPIENTS_PATH):
    with open(filepath, "r", encoding="utf-8") as f:
        return json.load(f)

def load_rss_sources():
    with open(RSS_SOURCES_FILE, "r", encoding="utf-8") as f:
        all_sources = json.load(f)

    # âœ… ëª¨ë“  êµ¬ë…ìì˜ ê´€ì‹¬ ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ
    recipients = load_recipients()
    interested_keys = set()
    for person in recipients:
        for key in person.get("categories", []):
            interested_keys.add(key)  # ì˜ˆ: "ë§¤ì¼ê²½ì œ::êµ­ì œ"

    # âœ… ê´€ì‹¬ìˆëŠ” ì¹´í…Œê³ ë¦¬ì— í•´ë‹¹í•˜ëŠ” RSSë§Œ í•„í„°ë§
    filtered_sources = []
    for entry in all_sources:
        key = f"{entry['source']}::{entry['category']}"
        if key in interested_keys:
            filtered_sources.append(entry)

    return filtered_sources

rss_sources = load_rss_sources()

news_data = []
existing_urls = get_existing_urls_from_notion(notion_token, notion_database_id)

logger.info("ë…¸ì…˜ ê°€ì ¸ì˜¤ê¸° ì™„ë£Œ")

for item in rss_sources:
    source = item["source"]  # ì˜ˆ: "ë§¤ì¼ê²½ì œ - ê²½ì œ"
    category = item["category"]
    rss_url = item["url"]

    print(f"{source} - {category} - {rss_url}")
    
    urls = get_latest_news_urls(rss_url)

    for url in urls:

        logger.info(f"ğŸŒ [{category}] í¬ë¡¤ë§ ì‹œì‘: {url}")

        
        article = crawl_news(url)

        if "Content not found" in article['content'] or article['title'] == "ERROR":
            logger.warning(f"â›” í¬ë¡¤ë§ ì‹¤íŒ¨ë¡œ ì œì™¸ë¨: {article['url']}")
            continue

        logger.info(f"âœ… í¬ë¡¤ë§ ì„±ê³µ: {article['title']}")

        if article['url'] in existing_urls:
            continue  # ì´ë¯¸ ì €ì¥ëœ ë‰´ìŠ¤ëŠ” ìŠ¤í‚µ

        try:
            result = summarize_news_via_api(article['title'], article['content'], api_key)

            # ê²°ê³¼ê°€ íŠœí”Œì´ ì•„ë‹ˆê±°ë‚˜, ê¸¸ì´ê°€ 3ì´ ì•„ë‹ˆë©´ ìŠ¤í‚µ
            # if not isinstance(result, tuple) or len(result) != 3:
            #     logger.warning(f"âš ï¸ ìš”ì•½ í˜•ì‹ ì´ìƒìœ¼ë¡œ ì œì™¸ë¨: {article['url']}, result={result}")
            #     continue

            summary, tags, emoji, is_ad = result

            # ìš”ì•½ ì‹¤íŒ¨ì¸ ê²½ìš°ë„ ê±´ë„ˆëœ€
            if summary == "ìš”ì•½ ì‹¤íŒ¨":
                logger.warning(f"âš ï¸ ìš”ì•½ ì‹¤íŒ¨ë¡œ ì œì™¸ë¨: {article['url']}")
                continue

        except Exception as e:
            logger.error(f"âŒ ìš”ì•½ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {article['url']} / {str(e)}", exc_info=True)
            continue

        article['category'] = category
        article['source'] = source
        
        article['summary'] = summary
        article['tags'] = tags
        article['emoji'] = emoji
        article['is_ad'] = is_ad
        
        

        news_data.append(article)

        logger.info(f"ğŸ¯ ìš”ì•½ ì™„ë£Œ ë° ê¸°ì‚¬ ì¶”ê°€ë¨: {article['url']}")

# âœ… ë…¸ì…˜ ì €ì¥
for article in news_data:
    save_to_notion(article, notion_token, notion_database_id)

# âœ… ë©”ì¼ + í…”ë ˆê·¸ë¨ ë°œì†¡
send_bulk_email(news_data)

logger.info("ë‰´ìŠ¤ë ˆí„° ë°œì†¡ ì‘ì—… ì™„ë£Œ!!!")