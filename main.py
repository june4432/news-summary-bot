### ğŸ“ main.py
from rss import get_latest_news_urls
from crawler import crawl_news
from summarizer import summarize_news_via_api
from notion_writer import save_to_notion, get_existing_urls_from_notion
from mailer import build_email_body, send_email, get_email_subject
from bulk_mailer import send_bulk_email
from config import api_key, notion_token, notion_database_id, sender_email, sender_app_password, recipient_email, notion_url  # recipient_email can be comma-separated
import datetime
from log import logger

logger.info("ë‰´ìŠ¤ë ˆí„° ë°œì†¡ ì‘ì—… ì‹œì‘!!")
rss_sources = [
    ("í—¤ë“œë¼ì¸", "https://www.mk.co.kr/rss/30000001/"),
    ("ê²½ì œ", "https://www.mk.co.kr/rss/30100041/"),
    ("êµ­ì œ", "https://www.mk.co.kr/rss/30300018/")
]

news_data = []
existing_urls = get_existing_urls_from_notion(notion_token, notion_database_id)

for category, rss_url in rss_sources:
    urls = get_latest_news_urls(rss_url)

    for url in urls:
        article = crawl_news(url)

        if "Content not found" in article['content']:
            logger.warning(f"â›” í¬ë¡¤ë§ ì‹¤íŒ¨ë¡œ ì œì™¸ë¨: {url}")
            continue  # ëª©ë¡ì—ì„œ ì œì™¸

        if article['url'] in existing_urls:
            continue  # ì´ë¯¸ ë³´ë‚¸ ë‰´ìŠ¤ëŠ” ìŠ¤í‚µ

        result = summarize_news_via_api(article['title'], article['content'], api_key)
        if isinstance(result, tuple) and len(result) == 2:
            summary, tags = result
        else:
            summary, tags = result, []
        article['summary'] = summary
        article['tags'] = tags
        article['category'] = category  # âœ… ì¹´í…Œê³ ë¦¬ ì¶”ê°€
        news_data.append(article)

for article in news_data:
    save_to_notion(article, notion_token, notion_database_id)

send_bulk_email(news_data)

logger.info("ë‰´ìŠ¤ë ˆí„° ë°œì†¡ ì‘ì—… ì™„ë£Œ!!!")