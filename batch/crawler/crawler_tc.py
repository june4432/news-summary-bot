import requests
import re
from bs4 import BeautifulSoup

# TechCrunch ë‰´ìŠ¤ í¬ë¡¤ëŸ¬
def crawl_tc_news(url):
    headers = {
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/124.0.0.0 Safari/537.36"
        )
    }

    try:
        response = requests.get(url, headers=headers)
        response.raise_for_status()
    except Exception as e:
        return {
            "title": "ERROR",
            "content": f"ìš”ì²­ ì‹¤íŒ¨: {e}",
            "images": [],
            "url": url
        }

    soup = BeautifulSoup(response.content, "html.parser")

    # âœ… ì œëª© ì¶”ì¶œ
    title_tag = soup.find("h1", class_="wp-block-post-title")
    if not title_tag:
        # ë‹¤ë¥¸ íŒ¨í„´ì˜ ì œëª© íƒœê·¸ ì‹œë„
        title_tag = soup.find("h1") or soup.find("title")
    
    title = title_tag.get_text(strip=True) if title_tag else "Title not found"
    
    # TechCrunchì—ì„œ ì‚¬ì´íŠ¸ëª… ì œê±°
    if "| TechCrunch" in title:
        title = title.replace("| TechCrunch", "").strip()

    content_parts = []
    image_urls = []
    image_count = 1

    # âœ… ë³¸ë¬¸ ì¶”ì¶œ - TechCrunch êµ¬ì¡°ì— ë§ì¶° div.entry-contentì—ì„œ ì¶”ì¶œ
    content_div = soup.select_one("div.entry-content")
    
    if content_div:
        # ëŒ€í‘œ ì´ë¯¸ì§€ë§Œ ì²˜ë¦¬ (wp-block-post-featured-image í´ë˜ìŠ¤)
        featured_image_figure = soup.find("figure", class_="wp-block-post-featured-image")
        if featured_image_figure:
            img_tag = featured_image_figure.find("img")
            if img_tag and img_tag.get("src"):
                src = img_tag.get("src")
                alt = img_tag.get("alt", "Featured Image")
                
                image_urls.append(src)
                # ë³¸ë¬¸ì— ì‚¬ì§„ í‘œì‹œ ì¶”ê°€
                content_parts.append(f"[ì‚¬ì§„{image_count}]")
                image_count += 1

        # ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ - ì§ì ‘ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        # ë¶ˆí•„ìš”í•œ ìš”ì†Œë“¤ ì œê±°
        for unwanted in content_div.find_all([
            "script", "style", "aside", "nav", "header", "footer",
            "figure", "img"  # ì´ë¯¸ì§€ëŠ” ì´ë¯¸ ì²˜ë¦¬í–ˆìœ¼ë¯€ë¡œ ì œê±°
        ]):
            unwanted.decompose()
        
        # ğŸš« TechCrunch ê´‘ê³ ì„± ìš”ì†Œë“¤ ì œê±°
        cta_elements = content_div.find_all(class_="wp-block-techcrunch-inline-cta")
        if cta_elements:
            print(f"ğŸš« TechCrunch CTA ìš”ì†Œ {len(cta_elements)}ê°œ ì œê±°ë¨")
            for ad_element in cta_elements:
                ad_element.decompose()
        
        # ğŸš« ê¸°íƒ€ ê´‘ê³ ì„± í´ë˜ìŠ¤ë“¤ë„ ì œê±°
        ad_classes = [
            "wp-block-embed",
            "wp-block-button", 
            "newsletter-signup",
            "inline-ad"
        ]
        total_ads_removed = 0
        for ad_class in ad_classes:
            ad_elements = content_div.find_all(class_=ad_class)
            if ad_elements:
                print(f"ğŸš« {ad_class} ê´‘ê³  ìš”ì†Œ {len(ad_elements)}ê°œ ì œê±°ë¨")
                total_ads_removed += len(ad_elements)
                for ad_element in ad_elements:
                    ad_element.decompose()
        
        if total_ads_removed > 0:
            print(f"ğŸš« ì´ {total_ads_removed}ê°œì˜ ê´‘ê³ ì„± ìš”ì†Œê°€ ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤")
        
        # ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜¨ í›„ ì •ë¦¬ (íƒœê·¸ ê²½ê³„ì—ì„œ ê³µë°±ìœ¼ë¡œ ì²˜ë¦¬)
        full_text = content_div.get_text(separator=" ", strip=True)
        
        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬ (ë§ˆì¹¨í‘œ, ëŠë‚Œí‘œ, ë¬¼ìŒí‘œ ê¸°ì¤€)
        sentences = re.split(r'(?<=[.!?])\s+', full_text)
        
        lines = []
        previous_sentence = ""
        
        for sentence in sentences:
            sentence = sentence.strip()
            # ì˜ë¯¸ìˆëŠ” ë¬¸ì¥ë§Œ í¬í•¨ (ê¸¸ì´ ì²´í¬ ë° íŠ¹ì • íŒ¨í„´ ì œì™¸)
            if (sentence and len(sentence) > 5 and  # ìµœì†Œ ê¸¸ì´ë¥¼ 15ì—ì„œ 5ë¡œ ì¤„ì„
                not sentence.lower().startswith(('image credits:', 'posted:', 'topics', 'photo by', 'image:', 'credit:')) and
                not sentence.endswith('ago') and
                not sentence.isdigit() and
                not re.match(r'^[0-9\s\-\/]+$', sentence)):  # ë‚ ì§œë‚˜ ìˆ«ìë§Œ ìˆëŠ” ì¤„ ì œì™¸
                
                # ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ ì •ë¦¬
                sentence = re.sub(r'\s+', ' ', sentence)
                
                # ì§§ì€ ë¬¸ì¥ ì²˜ë¦¬ - ì„¹ì…˜ ì œëª©ì´ë‚˜ ë¶ˆì™„ì „í•œ ë¬¸ì¥ë§Œ í•©ì¹˜ê¸°
                if (len(sentence) < 50 and lines and 
                    (not sentence.endswith(('.', '!', '?')) or  # ë¬¸ì¥ ë¶€í˜¸ë¡œ ëë‚˜ì§€ ì•Šê±°ë‚˜
                     sentence.isupper() or  # ëŒ€ë¬¸ìë¡œë§Œ ì´ë£¨ì–´ì ¸ ìˆê±°ë‚˜ (ì œëª©)
                     sentence.count(' ') < 3)):  # ë‹¨ì–´ê°€ 3ê°œ ë¯¸ë§Œì¸ ê²½ìš° (ì œëª©ì´ë‚˜ ë¶ˆì™„ì „í•œ ë¬¸ì¥)
                    lines[-1] = lines[-1] + " " + sentence
                else:
                    lines.append(sentence)
                    previous_sentence = sentence
        
        # ì¤‘ë³µ ì œê±°í•˜ë©´ì„œ ìˆœì„œ ìœ ì§€
        seen = set()
        unique_lines = []
        for line in lines:
            if line not in seen:
                seen.add(line)
                unique_lines.append(line)
        
        content_parts.extend(unique_lines)
    
    # ë³¸ë¬¸ì´ ì¶”ì¶œë˜ì§€ ì•Šì€ ê²½ìš° ëŒ€ì•ˆ ì‹œë„
    if not content_parts or (len(content_parts) == 1 and content_parts[0] == ""):
        # ì „ì²´ article íƒœê·¸ì—ì„œ ì¶”ì¶œ ì‹œë„
        article = soup.find("article")
        if article:
            # í—¤ë”, í‘¸í„°, ì‚¬ì´ë“œë°” ë“± ì œê±°
            for unwanted in article.find_all(["header", "footer", "aside", "nav", "script", "style"]):
                unwanted.decompose()
            
            paragraphs = article.find_all("p")
            for p in paragraphs:
                text = p.get_text(strip=True)
                if text and len(text) > 10:
                    content_parts.append(text)

    # ìµœì¢…ì ìœ¼ë¡œ ë‚´ìš©ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ë©”ì‹œì§€
    if not content_parts:
        content_parts = ["Content not found"]

    return {
        "title": title,
        "content": "\n\n".join(content_parts),
        "images": image_urls,
        "url": url
    } 